{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "636a922a-8359-4a88-98a8-5258aa092fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-22 00:37:19.518906: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-22 00:37:19.705498: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-22 00:37:19.705562: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-22 00:37:19.739604: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-22 00:37:19.812013: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-22 00:37:19.813050: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-22 00:37:20.744518: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "326a30a2-08f8-45ad-a90b-c2371622db43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12 16 21 24 41 43 15]\n",
      " [14 33 34 35 37 40  4]\n",
      " [ 1 12 16 19 23 43 34]\n",
      " [ 8 14 28 29 34 40 12]\n",
      " [ 6  7 15 22 26 40 41]\n",
      " [10 17 22 30 35 43 44]\n",
      " [ 7 18 19 26 33 45 37]\n",
      " [ 6 20 23 24 28 30 44]\n",
      " [12 19 21 29 40 45  1]\n",
      " [ 4 18 31 37 42 43 40]\n",
      " [11 21 22 30 39 44 31]\n",
      " [13 14 18 21 34 44 16]\n",
      " [11 16 25 27 35 36 37]\n",
      " [ 4  7 17 18 38 44 36]\n",
      " [ 8 12 13 29 33 42  5]\n",
      " [ 3  7 14 15 22 38 17]\n",
      " [21 26 27 32 34 42 31]\n",
      " [ 1  9 16 23 24 38 17]\n",
      " [13 16 23 31 36 44 38]\n",
      " [ 4  8 18 24 37 45  6]]\n"
     ]
    }
   ],
   "source": [
    "# Load all datasets from './data_sets'\n",
    "data_path = './data_sets'\n",
    "input_datasets = []\n",
    "output_datasets = []\n",
    "\n",
    "# Assuming your files are named input_data_1.csv, input_data_2.csv, ..., output_data_1.csv, output_data_2.csv, ...\n",
    "for i in range(1, 501):\n",
    "    input_file = os.path.join(data_path, f'input_data_{i:04d}.csv')\n",
    "    output_file = os.path.join(data_path, f'output_data_{i:04d}.csv')\n",
    "\n",
    "    # Load data using pandas\n",
    "    # input_data = pd.read_csv(input_file, delimiter=',', dtype=int).values\n",
    "    # output_data = pd.read_csv(output_file, delimiter=','dtype=int).values\n",
    "    input_data = np.genfromtxt(input_file, delimiter=',', dtype=int)\n",
    "    output_data = np.genfromtxt(output_file, delimiter=',', dtype=int)\n",
    "\n",
    "    input_datasets.append(input_data)\n",
    "    output_datasets.append(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ead893c9-8ad5-4dcd-82e0-4da08db3f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to NumPy arrays\n",
    "X = np.array(input_datasets, dtype=float)\n",
    "y = np.array(output_datasets, dtype=float)\n",
    "\n",
    "# Reshape y to be a 2D array\n",
    "# y = y.reshape(-1, 1)\n",
    "\n",
    "# Normalize the input data\n",
    "# scaler_X = StandardScaler()\n",
    "# X = scaler_X.fit_transform(X)\n",
    "scaler_X = 45.0\n",
    "X = X/scaler_X\n",
    "\n",
    "# Normalize the output data\n",
    "# scaler_y = StandardScaler()\n",
    "# y = scaler_y.fit_transform(y)\n",
    "scaler_y = 45.0\n",
    "y = y/scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ad57473-e2f8-4a06-b2e0-f7a19db50f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 500\n"
     ]
    }
   ],
   "source": [
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5dab12c-330b-4f25-a561-f8fb2b2d6521",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "13/13 [==============================] - 1s 16ms/step - loss: 0.3287 - val_loss: 0.2112\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.1578 - val_loss: 0.0911\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0552 - val_loss: 0.0236\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0269 - val_loss: 0.0222\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0243 - val_loss: 0.0218\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0244 - val_loss: 0.0210\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0238 - val_loss: 0.0210\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0236 - val_loss: 0.0210\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0251 - val_loss: 0.0212\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0245 - val_loss: 0.0216\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0237 - val_loss: 0.0210\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0234 - val_loss: 0.0207\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0234 - val_loss: 0.0208\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0234 - val_loss: 0.0211\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0233 - val_loss: 0.0206\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0233 - val_loss: 0.0205\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0234 - val_loss: 0.0206\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0233 - val_loss: 0.0208\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0235 - val_loss: 0.0212\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0233 - val_loss: 0.0206\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0230 - val_loss: 0.0203\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0229 - val_loss: 0.0212\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0234 - val_loss: 0.0209\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0228 - val_loss: 0.0204\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0235 - val_loss: 0.0211\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0227 - val_loss: 0.0205\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0229 - val_loss: 0.0206\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0228 - val_loss: 0.0204\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0227 - val_loss: 0.0207\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0228 - val_loss: 0.0204\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0226 - val_loss: 0.0202\n",
      "Epoch 32/50\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0226 - val_loss: 0.0217\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0231 - val_loss: 0.0202\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0225 - val_loss: 0.0203\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0225 - val_loss: 0.0203\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0225 - val_loss: 0.0205\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0228 - val_loss: 0.0203\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0231 - val_loss: 0.0203\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0223 - val_loss: 0.0202\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0224 - val_loss: 0.0207\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0224 - val_loss: 0.0211\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0222 - val_loss: 0.0209\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0227 - val_loss: 0.0202\n",
      "Epoch 44/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0223 - val_loss: 0.0202\n",
      "Epoch 45/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0221 - val_loss: 0.0203\n",
      "Epoch 46/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0220 - val_loss: 0.0213\n",
      "Epoch 47/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0234 - val_loss: 0.0213\n",
      "Epoch 48/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0228 - val_loss: 0.0206\n",
      "Epoch 49/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0223 - val_loss: 0.0206\n",
      "Epoch 50/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0220 - val_loss: 0.0205\n",
      "Mean Squared Error on Test Set: 0.02048446238040924\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "Predictions:\n",
      "[[0.15752725 0.2868528  0.4550627  0.60243994 0.72709674 0.8940532 ]\n",
      " [0.15040113 0.29259488 0.43707794 0.5717197  0.70359856 0.8330842 ]\n",
      " [0.1576057  0.2582896  0.439978   0.5705848  0.7036763  0.8685255 ]\n",
      " [0.15098767 0.2935432  0.43029892 0.5747674  0.7261053  0.8674392 ]\n",
      " [0.11409663 0.22801201 0.38725275 0.50981253 0.64408296 0.78198075]\n",
      " [0.15715143 0.28028256 0.43238026 0.5704032  0.7025426  0.86856556]\n",
      " [0.1479799  0.27265373 0.44118196 0.57238656 0.7148007  0.8688587 ]\n",
      " [0.14020465 0.26740307 0.42196625 0.55355936 0.7048579  0.83290577]\n",
      " [0.14880173 0.29402113 0.43097115 0.573096   0.710187   0.8602672 ]\n",
      " [0.14295202 0.27792457 0.39921618 0.52440137 0.6775834  0.7996243 ]\n",
      " [0.15285693 0.2800468  0.44847077 0.5881568  0.71250135 0.8706478 ]\n",
      " [0.14303888 0.27110305 0.41445613 0.57420665 0.7175346  0.8659443 ]\n",
      " [0.1657548  0.29181948 0.43114984 0.57452554 0.7136086  0.86879045]\n",
      " [0.14582953 0.2565522  0.431669   0.5480116  0.6882641  0.8377168 ]\n",
      " [0.15884958 0.2828258  0.4397121  0.5840448  0.7340779  0.8848935 ]\n",
      " [0.16742004 0.31399858 0.4395368  0.61258525 0.73334605 0.894468  ]\n",
      " [0.1524866  0.3000665  0.43429458 0.5865982  0.7293486  0.8732271 ]\n",
      " [0.15248214 0.28329954 0.43015826 0.5740558  0.71240956 0.85472625]\n",
      " [0.12254386 0.24737008 0.40607035 0.53920525 0.68233067 0.8339183 ]\n",
      " [0.15835613 0.28417596 0.4216634  0.5692968  0.7023684  0.8523235 ]\n",
      " [0.14638083 0.27120477 0.41510367 0.56276447 0.6922528  0.8463799 ]\n",
      " [0.12072334 0.24883583 0.37865543 0.53625536 0.6547467  0.80256784]\n",
      " [0.14511712 0.26123247 0.42070204 0.56853336 0.684684   0.86195415]\n",
      " [0.16991901 0.29036665 0.44176602 0.601413   0.7285205  0.88825154]\n",
      " [0.16459344 0.3057736  0.43019295 0.5895105  0.73213524 0.8681238 ]\n",
      " [0.13825166 0.2430991  0.4005223  0.536243   0.66022307 0.8020064 ]\n",
      " [0.14105107 0.25773522 0.4248761  0.578383   0.7070481  0.8745752 ]\n",
      " [0.17145303 0.30631563 0.43771875 0.59916455 0.72618264 0.8911152 ]\n",
      " [0.15582225 0.28794676 0.41436964 0.54279554 0.69154143 0.8409245 ]\n",
      " [0.14216958 0.25971487 0.41426337 0.53798634 0.6707376  0.80445445]\n",
      " [0.1492162  0.27189845 0.41665822 0.54250586 0.68272406 0.8291999 ]\n",
      " [0.13725293 0.26738164 0.4097708  0.53287375 0.67150885 0.81076175]\n",
      " [0.14937685 0.3045285  0.42652822 0.5666175  0.70954806 0.84099984]\n",
      " [0.1436377  0.27085662 0.4333307  0.56720716 0.700036   0.851779  ]\n",
      " [0.14623894 0.2798468  0.44402778 0.5762042  0.71306735 0.8776455 ]\n",
      " [0.13202049 0.25510073 0.40720087 0.52217364 0.662162   0.77250665]\n",
      " [0.14026807 0.26622987 0.40979314 0.54909354 0.684417   0.82421166]\n",
      " [0.16238607 0.30189314 0.4454512  0.60980535 0.7451701  0.895393  ]\n",
      " [0.14230238 0.27329832 0.4217695  0.5597359  0.69737047 0.8384261 ]\n",
      " [0.17082448 0.32789302 0.45214963 0.63121337 0.74392396 0.9245463 ]\n",
      " [0.1122882  0.23122936 0.37628043 0.50167924 0.6408925  0.78180695]\n",
      " [0.16924518 0.30013123 0.42731845 0.58206373 0.7151911  0.86910665]\n",
      " [0.13034074 0.2424298  0.40006047 0.5302819  0.6684746  0.81124526]\n",
      " [0.16326164 0.324153   0.43932098 0.59640026 0.7310634  0.8941747 ]\n",
      " [0.16420288 0.30397138 0.43992972 0.5903837  0.72267205 0.8887881 ]\n",
      " [0.16639642 0.297452   0.42374724 0.5688187  0.7175684  0.85518104]\n",
      " [0.1664619  0.28503707 0.4442526  0.5882733  0.72517174 0.89207506]\n",
      " [0.13818437 0.2718417  0.4272262  0.55523413 0.7000181  0.8225424 ]\n",
      " [0.14596158 0.27470255 0.42535424 0.55525404 0.7112027  0.8474888 ]\n",
      " [0.16959761 0.3042024  0.43177348 0.5962842  0.7273802  0.89285356]\n",
      " [0.163782   0.3033499  0.44023192 0.5900885  0.7151324  0.86546886]\n",
      " [0.15384863 0.28711373 0.4296143  0.5761283  0.7129862  0.85969985]\n",
      " [0.15654548 0.29132968 0.44608527 0.5932823  0.74877775 0.87290543]\n",
      " [0.12150525 0.24028294 0.3763526  0.50851834 0.64098245 0.7708455 ]\n",
      " [0.16008328 0.2875968  0.44261122 0.59764844 0.74637216 0.8862678 ]\n",
      " [0.1393797  0.26705435 0.39944977 0.5388505  0.6749171  0.81211543]\n",
      " [0.16276468 0.31368694 0.44508803 0.60190445 0.7387504  0.8985269 ]\n",
      " [0.1513702  0.2766373  0.42562902 0.56565166 0.7019821  0.8586766 ]\n",
      " [0.15986414 0.2954905  0.42135376 0.56465846 0.6982356  0.8572144 ]\n",
      " [0.16413768 0.2873679  0.44063646 0.5860692  0.7293602  0.8734933 ]\n",
      " [0.14752342 0.2913558  0.43451732 0.5793076  0.72463405 0.85814476]\n",
      " [0.15565476 0.29912567 0.41317815 0.55145806 0.7005839  0.8297456 ]\n",
      " [0.15395324 0.3045043  0.43949866 0.5957821  0.7386873  0.8678651 ]\n",
      " [0.15088978 0.27893293 0.41470414 0.5506499  0.70071226 0.8342812 ]\n",
      " [0.15692142 0.2772003  0.43712002 0.5726059  0.7056586  0.844643  ]\n",
      " [0.13256465 0.2555299  0.41908246 0.53159994 0.66845435 0.8013682 ]\n",
      " [0.1405584  0.26779705 0.3961091  0.521874   0.67702967 0.7955177 ]\n",
      " [0.14547685 0.26807657 0.4200377  0.5606998  0.70375514 0.84822315]\n",
      " [0.16350468 0.30806878 0.45551538 0.6071319  0.7479558  0.9020941 ]\n",
      " [0.13400955 0.26173592 0.41897786 0.56702864 0.687584   0.8653177 ]\n",
      " [0.1400387  0.24791148 0.40122062 0.52763253 0.65516394 0.8027703 ]\n",
      " [0.15753725 0.30863628 0.41673625 0.5703905  0.6923893  0.843354  ]\n",
      " [0.15391381 0.293812   0.41063124 0.5454928  0.6969206  0.8366828 ]\n",
      " [0.15642232 0.2643159  0.4257025  0.5520871  0.69080466 0.85462743]\n",
      " [0.15218006 0.27851284 0.44871807 0.5826594  0.7351769  0.8928725 ]\n",
      " [0.15692219 0.2873166  0.4189927  0.5587093  0.7095496  0.8341375 ]\n",
      " [0.15459795 0.28278992 0.41609895 0.55605966 0.70646876 0.85218275]\n",
      " [0.14877008 0.29077053 0.41962898 0.57418996 0.71014386 0.8618647 ]\n",
      " [0.14198105 0.26632985 0.4122424  0.54695076 0.6908974  0.81151426]\n",
      " [0.14919175 0.2683854  0.42193717 0.55937904 0.6896468  0.8573482 ]\n",
      " [0.1396627  0.29038888 0.42814994 0.57050586 0.72628355 0.84348285]\n",
      " [0.16141464 0.2702673  0.42615455 0.55909675 0.69281995 0.8465407 ]\n",
      " [0.14473961 0.26547137 0.42723948 0.5452607  0.68448895 0.8275968 ]\n",
      " [0.14821741 0.281567   0.4366765  0.5722611  0.6979313  0.8560588 ]\n",
      " [0.15365146 0.29773185 0.4267658  0.57739145 0.72748804 0.88887835]\n",
      " [0.12851134 0.25059482 0.40545315 0.5501438  0.6676724  0.82781863]\n",
      " [0.143235   0.28584766 0.41121918 0.56518435 0.69169706 0.8441121 ]\n",
      " [0.14464438 0.28818598 0.41135693 0.5484978  0.70713645 0.8239888 ]\n",
      " [0.13685928 0.28461504 0.41138685 0.5606274  0.70352155 0.8598231 ]\n",
      " [0.15827532 0.27957296 0.4183628  0.5511355  0.69883245 0.8522909 ]\n",
      " [0.14057021 0.25545707 0.4163763  0.5343915  0.6789953  0.8205265 ]\n",
      " [0.15121502 0.27340972 0.4264853  0.55975634 0.70148605 0.8494941 ]\n",
      " [0.14601211 0.2773891  0.4451695  0.5803905  0.71373755 0.8614814 ]\n",
      " [0.12858845 0.27063453 0.42600638 0.5706614  0.69964105 0.8342868 ]\n",
      " [0.1497658  0.29637104 0.43358803 0.580693   0.721263   0.86961925]\n",
      " [0.14936034 0.27041754 0.43066114 0.5711048  0.69504094 0.85638404]\n",
      " [0.15048848 0.26581725 0.41101676 0.5501451  0.67454594 0.84166753]\n",
      " [0.1443754  0.27243116 0.44212526 0.58125967 0.7192725  0.8526048 ]\n",
      " [0.15325224 0.29628807 0.4190489  0.5778025  0.71182495 0.86265683]\n",
      " [0.14230941 0.2556054  0.4107721  0.5314004  0.66986066 0.79783547]]\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(20, 7)))\n",
    "model.add(Dense(6))  # Output layer with six neurons for regression\n",
    "model.compile(optimizer='adam', loss='mse')  # Mean Squared Error loss for regression\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Mean Squared Error on Test Set: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Print the predictions\n",
    "print('Predictions:')\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29351c98-434d-4794-9d8b-49c2393c0ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karl/anaconda3/envs/ailt00/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('model_lstm_i20_7_i1_6.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ailt00",
   "language": "python",
   "name": "ailt00"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
