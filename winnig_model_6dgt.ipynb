{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "636a922a-8359-4a88-98a8-5258aa092fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "326a30a2-08f8-45ad-a90b-c2371622db43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all datasets from './data_sets'\n",
    "data_path = './data_sets'\n",
    "input_datasets = []\n",
    "output_datasets = []\n",
    "\n",
    "# Assuming your files are named input_data_1.csv, input_data_2.csv, ..., output_data_1.csv, output_data_2.csv, ...\n",
    "for i in range(1, 501):\n",
    "    input_file = os.path.join(data_path, f'input_data_{i:04d}.csv')\n",
    "    output_file = os.path.join(data_path, f'output_data_{i:04d}.csv')\n",
    "\n",
    "    # Load data using pandas\n",
    "    # input_data = pd.read_csv(input_file, delimiter=',', dtype=int).values\n",
    "    # output_data = pd.read_csv(output_file, delimiter=','dtype=int).values\n",
    "    input_data = np.genfromtxt(input_file, delimiter=',', dtype=int)\n",
    "    output_data = np.genfromtxt(output_file, delimiter=',', dtype=int)\n",
    "\n",
    "    input_datasets.append(input_data)\n",
    "    output_datasets.append(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8385a870-7119-4b83-9096-a270cdd89a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 16 21 24 41 43]\n"
     ]
    }
   ],
   "source": [
    "print(output_datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ead893c9-8ad5-4dcd-82e0-4da08db3f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to NumPy arrays\n",
    "X = np.array(input_datasets, dtype=float)\n",
    "y = np.array(output_datasets, dtype=float)\n",
    "\n",
    "# Reshape y to be a 2D array\n",
    "# y = y.reshape(-1, 1)\n",
    "\n",
    "# Normalize the input data\n",
    "# scaler_X = StandardScaler()\n",
    "# X = scaler_X.fit_transform(X)\n",
    "scaler_X = 45.0\n",
    "X = X/scaler_X\n",
    "\n",
    "# Normalize the output data\n",
    "# scaler_y = StandardScaler()\n",
    "# y = scaler_y.fit_transform(y)\n",
    "scaler_y = 45.0\n",
    "y = y/scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ad57473-e2f8-4a06-b2e0-f7a19db50f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 500\n"
     ]
    }
   ],
   "source": [
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5dab12c-330b-4f25-a561-f8fb2b2d6521",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "13/13 [==============================] - 1s 18ms/step - loss: 0.2977 - val_loss: 0.1709\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.1188 - val_loss: 0.0701\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0470 - val_loss: 0.0242\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0247 - val_loss: 0.0221\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0242 - val_loss: 0.0209\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0237 - val_loss: 0.0210\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0236 - val_loss: 0.0225\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0239 - val_loss: 0.0207\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0233 - val_loss: 0.0208\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0232 - val_loss: 0.0211\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0236 - val_loss: 0.0206\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0239 - val_loss: 0.0211\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0232 - val_loss: 0.0207\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0233 - val_loss: 0.0209\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0231 - val_loss: 0.0206\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0232 - val_loss: 0.0211\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0232 - val_loss: 0.0207\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0227 - val_loss: 0.0207\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0231 - val_loss: 0.0210\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0231 - val_loss: 0.0203\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0228 - val_loss: 0.0208\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0228 - val_loss: 0.0203\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0227 - val_loss: 0.0205\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0227 - val_loss: 0.0207\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0229 - val_loss: 0.0207\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0234 - val_loss: 0.0205\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0232 - val_loss: 0.0217\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0229 - val_loss: 0.0205\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0226 - val_loss: 0.0203\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0227 - val_loss: 0.0204\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0224 - val_loss: 0.0204\n",
      "Epoch 32/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0224 - val_loss: 0.0212\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0226 - val_loss: 0.0204\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0225 - val_loss: 0.0203\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0226 - val_loss: 0.0203\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0225 - val_loss: 0.0204\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0223 - val_loss: 0.0204\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0222 - val_loss: 0.0205\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0224 - val_loss: 0.0203\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0222 - val_loss: 0.0204\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0225 - val_loss: 0.0218\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0233 - val_loss: 0.0210\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0228 - val_loss: 0.0214\n",
      "Epoch 44/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0228 - val_loss: 0.0212\n",
      "Epoch 45/50\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0229 - val_loss: 0.0204\n",
      "Epoch 46/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0228 - val_loss: 0.0215\n",
      "Epoch 47/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0227 - val_loss: 0.0203\n",
      "Epoch 48/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0221 - val_loss: 0.0207\n",
      "Epoch 49/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0224 - val_loss: 0.0207\n",
      "Epoch 50/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0225 - val_loss: 0.0207\n",
      "Mean Squared Error on Test Set: 0.02067316137254238\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "Predictions:\n",
      "[[0.16959319 0.35275397 0.4677881  0.61961037 0.78127    0.9391524 ]\n",
      " [0.16034296 0.30894575 0.44141713 0.5767285  0.74328095 0.8881005 ]\n",
      " [0.15832224 0.3255229  0.45462155 0.5932694  0.76207584 0.9244631 ]\n",
      " [0.17632833 0.3326162  0.45869482 0.6100963  0.75579387 0.9012798 ]\n",
      " [0.13883767 0.25524005 0.3999068  0.53224057 0.68966144 0.842326  ]\n",
      " [0.17405647 0.32149586 0.44319397 0.58599454 0.7488047  0.9022205 ]\n",
      " [0.16645098 0.30981582 0.43242052 0.57456267 0.74382955 0.8940663 ]\n",
      " [0.16164547 0.32912004 0.45300677 0.59679854 0.75211054 0.8973767 ]\n",
      " [0.16101149 0.3146646  0.45571214 0.5907679  0.7644606  0.9198089 ]\n",
      " [0.16950819 0.29366282 0.42164376 0.55888724 0.70680195 0.84937584]\n",
      " [0.17044541 0.33666727 0.4506701  0.6015507  0.765353   0.9116685 ]\n",
      " [0.13741001 0.3077243  0.44226283 0.60088795 0.7591713  0.9272108 ]\n",
      " [0.18154925 0.34142387 0.46084797 0.6078255  0.75842386 0.90721256]\n",
      " [0.15527725 0.30785853 0.44265944 0.5698872  0.7447265  0.8939802 ]\n",
      " [0.1689649  0.3369623  0.46855548 0.6056657  0.76697046 0.9313963 ]\n",
      " [0.18835926 0.34013402 0.45740142 0.60379976 0.75802684 0.90845126]\n",
      " [0.16823187 0.32967162 0.4620088  0.6045991  0.7563512  0.9056488 ]\n",
      " [0.16201779 0.3133551  0.46043706 0.5976927  0.7530647  0.91330296]\n",
      " [0.14183882 0.28560576 0.4142912  0.56785756 0.7328741  0.8845832 ]\n",
      " [0.17315719 0.32367894 0.45416158 0.5912876  0.7574257  0.9062553 ]\n",
      " [0.16104323 0.31951764 0.44515833 0.5960489  0.7560974  0.90712565]\n",
      " [0.1232518  0.2613837  0.3914116  0.5527221  0.7087386  0.8631152 ]\n",
      " [0.16495565 0.3086791  0.43610287 0.58943504 0.747717   0.90843743]\n",
      " [0.1770227  0.34789968 0.4742603  0.6180528  0.7749746  0.93643016]\n",
      " [0.15194401 0.3365488  0.4725355  0.61370647 0.76787084 0.9239231 ]\n",
      " [0.16006148 0.29755133 0.4115524  0.56284195 0.72595984 0.8650455 ]\n",
      " [0.14522576 0.3025783  0.43698877 0.59045887 0.7532744  0.92221826]\n",
      " [0.17724344 0.3482991  0.47279412 0.6148123  0.7795753  0.93673104]\n",
      " [0.16187143 0.3111749  0.45022494 0.5854785  0.74752384 0.8970873 ]\n",
      " [0.16596055 0.29568538 0.42202318 0.5506834  0.7049193  0.8423098 ]\n",
      " [0.15305668 0.30672383 0.42662713 0.5644174  0.7234204  0.87347835]\n",
      " [0.15569422 0.2992356  0.43316305 0.56543916 0.721875   0.8675115 ]\n",
      " [0.16771746 0.31575254 0.44776076 0.5894533  0.75111943 0.8902909 ]\n",
      " [0.15171093 0.31610605 0.45854223 0.58926034 0.7571388  0.91156334]\n",
      " [0.15609887 0.3246011  0.46943796 0.60157174 0.78252417 0.94501555]\n",
      " [0.15158165 0.2811621  0.40532798 0.5341429  0.7047903  0.8309178 ]\n",
      " [0.15485138 0.30788583 0.44262478 0.57969147 0.7513358  0.8977906 ]\n",
      " [0.17411077 0.346453   0.48009133 0.62348115 0.7861173  0.9457328 ]\n",
      " [0.14148289 0.3105135  0.45274132 0.5854108  0.7546169  0.91939944]\n",
      " [0.19697553 0.35565475 0.48087266 0.6305377  0.78085965 0.94143766]\n",
      " [0.1181658  0.23988608 0.36760116 0.51725614 0.6705045  0.8188018 ]\n",
      " [0.16031477 0.33051693 0.46569335 0.59949344 0.77138776 0.9237368 ]\n",
      " [0.14532143 0.2801102  0.40648335 0.5547591  0.71266425 0.8628185 ]\n",
      " [0.17090091 0.3315398  0.46918613 0.61132246 0.7682591  0.9311721 ]\n",
      " [0.17043316 0.34194228 0.4706241  0.6147173  0.7749625  0.93198043]\n",
      " [0.15748766 0.32600597 0.4592072  0.60018003 0.76428074 0.9199979 ]\n",
      " [0.18500039 0.3348661  0.47006923 0.60998046 0.777428   0.9418424 ]\n",
      " [0.15287793 0.30914664 0.4306662  0.56824744 0.7408511  0.874828  ]\n",
      " [0.17052042 0.30550632 0.44132435 0.5789689  0.73669803 0.8851206 ]\n",
      " [0.17563018 0.3438289  0.47624105 0.6213942  0.7776118  0.9368888 ]\n",
      " [0.17128345 0.32773626 0.4523687  0.5941132  0.7599054  0.9148765 ]\n",
      " [0.15823963 0.3131862  0.4518535  0.584117   0.7471503  0.8993444 ]\n",
      " [0.16630375 0.33837363 0.4647966  0.60690826 0.77430445 0.9260171 ]\n",
      " [0.13433269 0.26018488 0.3923088  0.53797805 0.7068448  0.8494351 ]\n",
      " [0.16774988 0.35024747 0.480736   0.6316288  0.7910706  0.95040554]\n",
      " [0.1589207  0.28107414 0.40922278 0.55263263 0.7082136  0.8509068 ]\n",
      " [0.17416286 0.3489801  0.48284918 0.6249611  0.78303224 0.9404618 ]\n",
      " [0.17042789 0.33709994 0.46069312 0.6120014  0.74878246 0.90464264]\n",
      " [0.17431697 0.3270576  0.46094096 0.60065967 0.7474876  0.9001628 ]\n",
      " [0.16632423 0.33550724 0.46937895 0.6078229  0.76251835 0.9239957 ]\n",
      " [0.17581701 0.32696864 0.45538056 0.5922089  0.7433713  0.88570446]\n",
      " [0.16576007 0.3083794  0.4471057  0.5889112  0.7435192  0.8831813 ]\n",
      " [0.1875723  0.33467412 0.45203888 0.5977983  0.76621175 0.8994483 ]\n",
      " [0.15282512 0.31604943 0.44358844 0.5827023  0.74198335 0.8891676 ]\n",
      " [0.16179869 0.32094473 0.44716263 0.5862828  0.74970156 0.90286523]\n",
      " [0.15486395 0.28687793 0.42879832 0.5558745  0.7095385  0.85836905]\n",
      " [0.16361493 0.2971894  0.42709634 0.56677026 0.70730346 0.8507473 ]\n",
      " [0.17446798 0.31625214 0.4497949  0.5886376  0.74179417 0.8988839 ]\n",
      " [0.17006922 0.3458695  0.48738798 0.634786   0.79719746 0.9531485 ]\n",
      " [0.15926152 0.2883288  0.41104352 0.56678855 0.724033   0.89356065]\n",
      " [0.14365298 0.29276678 0.42482284 0.559097   0.7192864  0.868251  ]\n",
      " [0.16828844 0.31211415 0.441525   0.5845457  0.7391744  0.8848896 ]\n",
      " [0.16724333 0.31442583 0.45012146 0.58879715 0.7444101  0.8916053 ]\n",
      " [0.15645617 0.3123387  0.4396187  0.58072287 0.75260144 0.90686864]\n",
      " [0.16899303 0.32325068 0.4561417  0.60160875 0.7647485  0.9186369 ]\n",
      " [0.16262782 0.3116504  0.45252088 0.5900444  0.74454826 0.89137465]\n",
      " [0.16760749 0.31116846 0.46093327 0.5954466  0.7493557  0.91073817]\n",
      " [0.16337311 0.31544355 0.4508     0.5934727  0.75665814 0.9108109 ]\n",
      " [0.16077578 0.31443822 0.4435035  0.5903123  0.7243174  0.8631641 ]\n",
      " [0.17814964 0.31899592 0.44638568 0.5917259  0.7487615  0.9053069 ]\n",
      " [0.15710905 0.32606643 0.45860934 0.6049544  0.7513051  0.8929503 ]\n",
      " [0.1571913  0.31070593 0.4511689  0.57805467 0.74774307 0.9047857 ]\n",
      " [0.16805992 0.3168843  0.43255782 0.5777     0.7283452  0.8693682 ]\n",
      " [0.16132721 0.3224153  0.4575895  0.5935816  0.75946075 0.9108897 ]\n",
      " [0.16646454 0.32769313 0.46066284 0.6087818  0.7873973  0.9442398 ]\n",
      " [0.1251902  0.27682734 0.41034332 0.56093425 0.7219011  0.8853956 ]\n",
      " [0.14683983 0.28700536 0.4207363  0.5605297  0.7264666  0.87614745]\n",
      " [0.16123718 0.30411103 0.43877143 0.5722762  0.72569937 0.86353433]\n",
      " [0.15759361 0.29440925 0.41682056 0.575243   0.7197265  0.87434644]\n",
      " [0.1730769  0.3143263  0.44830436 0.58735156 0.7491649  0.9035589 ]\n",
      " [0.165755   0.28992704 0.42374325 0.549862   0.71678835 0.8604612 ]\n",
      " [0.15869054 0.31266704 0.4404109  0.5791796  0.75062215 0.9027316 ]\n",
      " [0.15356699 0.32563454 0.45226333 0.59249204 0.7682034  0.92423695]\n",
      " [0.15307733 0.289385   0.41147628 0.56796235 0.72842085 0.87363726]\n",
      " [0.1810261  0.33524427 0.44734645 0.60917246 0.76920193 0.9069802 ]\n",
      " [0.1661821  0.32049882 0.45633274 0.5989808  0.7445161  0.905341  ]\n",
      " [0.14295879 0.2941903  0.43054512 0.5699553  0.73318106 0.895096  ]\n",
      " [0.14474875 0.33183536 0.47158462 0.6107179  0.7620358  0.92055005]\n",
      " [0.16525885 0.31667873 0.44119048 0.5961572  0.76665205 0.9091191 ]\n",
      " [0.14983308 0.2997786  0.42333674 0.55605096 0.71418536 0.8527541 ]]\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(20, 7)))\n",
    "model.add(Dense(6))  # Output layer with six neurons for regression\n",
    "model.compile(optimizer='adam', loss='mse')  # Mean Squared Error loss for regression\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Mean Squared Error on Test Set: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Print the predictions\n",
    "print('Predictions:')\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ailt00",
   "language": "python",
   "name": "ailt00"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
